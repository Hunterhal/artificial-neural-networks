{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Multi Layer Perceptron\n",
    "\n",
    "![MLP](mlp.png)\n",
    "\n",
    "The multi layer perceptron is shown above. This MLP has 3 inputs 4 neurons at first layer, 2 neurons at second layer, and two neurons at output layer with tanh activation functions. Please refer to Perceptron/Adaline section. The circles with b are representing the bias term. All the connections inside the network are weights that are the learnable parameters including bias weights. The single weight is represented as $w_{current,in}^{layer}$. In this representation the current is the index of the neuron in the layer, in is the index of previous neurron or bias, and lastly layer is the index of the layer. For example, the purple connection is $w_{3,1}^{0}$ since it is the weight of neuron 3 in the layer 0 and connected to previous neuron 1. Likewise green connection is $w_{1,4}^{1}$ since it is the weight of neuron 1 in the layer 1 and connected to previous neuron 4 or bias."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Forward pass - Inference\n",
    "\n",
    "The forward pass through the multilayer perceptron or the term commonly used inference thorugh the network can be define with these three equations.\n",
    "\n",
    "![Layer0](layer0.png)\n",
    "\n",
    "![Layer1](layer1.png)\n",
    "\n",
    "![Layer2-Output](layer2.png)\n",
    "\n",
    "In these equations $v_{neuron}^{layer}$ is the linear combination, and the $y_{neuron}^{layer}$ is the output or the activation of the neuron. In order to keep consistency, tanh is used activation function. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.34347225 -0.37497088 -0.04825368  0.34198607]\n [ 0.21555413  0.2579497   0.34597178 -0.38122206]\n [-0.30182011  0.05749384  0.24651026  0.20541965]\n [-0.49271676 -0.24746719 -0.12805717 -0.39181037]] (4, 4)\n[[ 0.24476683 -0.44131624 -0.28024597 -0.35054271  0.24511947]\n [ 0.38303293  0.36387195 -0.25886926 -0.14149341  0.24035501]] (2, 5)\n[[-0.47292639 -0.49982257  0.16357976]\n [-0.44119917 -0.39276288  0.04815519]] (2, 3)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "input_dimension = 3\n",
    "input_data = np.random.rand(input_dimension + 1, 1) #bias added\n",
    "input_data[-1] = 1\n",
    "\n",
    "layer_0 = 4\n",
    "layer_1 = 2\n",
    "layer_out = 2\n",
    "\n",
    "ground_truth = np.random.rand(layer_out, 1)\n",
    "\n",
    "#Keep in mind the bias weights, all the weights are sampled from uniform distribution between 0 and 1. The substraction makes it -0.5 and 0.5.\n",
    "weights_0 = np.random.rand(layer_0, input_dimension + 1) - 0.5\n",
    "weights_1 = np.random.rand(layer_1, layer_0 + 1) - 0.5\n",
    "weights_out = np.random.rand(layer_out, layer_1 + 1) - 0.5\n",
    "\n",
    "print(weights_0, weights_0.shape)\n",
    "print(weights_1, weights_1.shape)\n",
    "print(weights_out, weights_out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4, 1)\n(5, 1)\n(3, 1)\n(2, 1)\n"
     ]
    }
   ],
   "source": [
    "#Now lets use inference to calculate outputs\n",
    "input_0 = input_data\n",
    "print(input_0.shape)\n",
    "lin_comb_0 = np.matmul(weights_0, input_0) #v\n",
    "output_0 = np.tanh(lin_comb_0) #y\n",
    "\n",
    "#Add bias to output 0\n",
    "input_1 = np.concatenate((output_0, np.ones((1,1))))\n",
    "print(input_1.shape)\n",
    "\n",
    "lin_comb_1 = np.matmul(weights_1, input_1) #v\n",
    "output_1 = np.tanh(lin_comb_1) #y\n",
    "\n",
    "#Add bias to output 1\n",
    "input_out = np.concatenate((output_1, np.ones((1,1))))\n",
    "print(input_out.shape)\n",
    "\n",
    "lin_comb_out = np.matmul(weights_out, input_out) #v\n",
    "output_out = np.tanh(lin_comb_out) #y\n",
    "print(output_out.shape)"
   ]
  },
  {
   "source": [
    "## Training\n",
    "\n",
    "### Mean Squared Error\n",
    "\n",
    "In training phase, weights are updated by using backpropagation algorithm. The mean squared error, between output of the adaline $y$ and the ground truth $y_d$, is calculated by using\n",
    "$$mse=\\frac{1}{n}\\overset{n}{\\sum_{j}}(y_{dj}-y_j)^2$$\n",
    "In this equation $n$ is the number of outputs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean squared error between [[0.16353365]\n [0.31125014]] and [[-0.32413905]\n [-0.36610679]] is 0.3483185349298162\n"
     ]
    }
   ],
   "source": [
    "def mean_square_error(ground_truth, desired):\n",
    "    temp = np.square(ground_truth - desired)\n",
    "    temp = np.mean( temp )\n",
    "    return temp\n",
    "\n",
    "print(\"Mean squared error between {} and {} is {}\".format(ground_truth, output_out, mean_square_error(ground_truth, output_out)))"
   ]
  },
  {
   "source": [
    "### Backpropagation\n",
    "\n",
    "#### Output Layer\n",
    "\n",
    "IMPORTAN NOTE: THE DERIVATIVES ARE FOR DEMONSTRATION ONLY, THEY ARE NOT ALLIGNED INITIALLY, AFTERWARDS THE MATRIX MULTIPLICATIONS AND ELEMENT-WISE OPERATIONS ARE EXPLAINED IN DETAIL. \n",
    "The ground truth is defined as the vector $y_d$, then the error can be defined as\n",
    "$$ e = \\begin{bmatrix} e_0 \\\\ e_1 \\end{bmatrix} = \\begin{bmatrix} y_{d,0} - y_0 \\\\ y_{d,1} - y_1 \\end{bmatrix}$$\n",
    "\n",
    "Becareful that the $E$ represents the means squared error.  \n",
    "Then the backpropagation term for the weights in output layer can be defined as\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w^{out}(k)}=\\frac{\\partial E}{\\partial e}\\frac{\\partial e}{\\partial y^{out}}\\frac{\\partial y^{out}}{\\partial v^{out}}\\frac{\\partial v^{out}}{\\partial w^{out}(k)}$$\n",
    "Each derivative term is defined as,  \n",
    "*Derivative of MSE: $E=\\frac{1}{2}e^2$ $\\frac{\\partial E}{\\partial e} = e$\n",
    "\n",
    "*Derivative of error term: $\\frac{\\partial e}{\\partial y^{out}} = -1$\n",
    "\n",
    "*Derivative of activation function: $\\frac{\\partial y^{out}}{\\partial v^{out}} = 1 - tanh^2(v^{out}) = 1 - (y^{out})^2$\n",
    "\n",
    "*Derivative of linear combination: $\\frac{\\partial v^{out}}{\\partial w^{out}(k)} = x^{out}$\n",
    "\n",
    "The $x^{out}$ is the output of the previous layer with bias = 1 concatanated to the end of the vector.\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w^{out}(k)}= e\\circ(-1)\\circ(1 - (y^{out})^2)\\times (x^{out})^T$$\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w^{out}(k)}= \\begin{bmatrix} e_0 \\\\ e_1 \\end {bmatrix}\\circ (-1)\\circ (1 - (\\begin{bmatrix} y^{out}_0 \\\\ y^{out}_1 \\end{bmatrix})^2) \\times \\begin{bmatrix} y^{1}_0 & y^{1}_1 & 1 \\end{bmatrix} $$\n",
    "\n",
    "The $e$ and $y^{out}$ multiplitaions is element-wise and the last one is matrix multiplitaion."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 1)\n(3, 1)\n(2, 3)\n"
     ]
    }
   ],
   "source": [
    "mse = mean_square_error(ground_truth, output_out)\r\n",
    "error = ground_truth - output_out \r\n",
    "\r\n",
    "temp = error * -1 * (1 - output_out**2)\r\n",
    "print(temp.shape)\r\n",
    "print(input_out.shape)\r\n",
    "output_derivative = np.matmul(temp.reshape(-1, 1), input_out.reshape(1, -1))\r\n",
    "print(output_derivative.shape)"
   ]
  },
  {
   "source": [
    "#### Hidden Layers\n",
    "\n",
    "In order compute the gradients of hidden layers, the local gradients term is come into play. If you check the multilayer perceptron schematic the red line shows a single way of data flow through MLP. Calculation of output layer only requires information from the error but to calculate weights between layer 1 and layer 0, $w^1$ now the both output neurons effect the outcome. So that change in these neurons must back propagated.\n",
    "\n",
    "Let's look at the derivative calculation, this calculation is demonstration only go below for matrix operations.\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w^{1}(k)}=\\frac{\\partial E}{\\partial e}\\frac{\\partial e}{\\partial y^{out}}\\frac{\\partial y^{out}}{\\partial v^{out}}\\frac{\\partial v^{out}}{\\partial y^{1}}\\frac{\\partial y^{1}}{\\partial v^{1}}\\frac{\\partial v^{1}}{\\partial w^{1}(k)}$$\n",
    "\n",
    "The strange thing here is the $\\frac{\\partial v^{out}}{\\partial y^{1}}$ term. The meaning of this term is how much linear combination of output neurons change with the inputs. The most important thing to understand is that the inputs of the output layer is the outcome of the previous layer. Then normal back propagation can continue. The terms are defined as\n",
    "\n",
    "* Local gradient: $\\delta^{out} = \\frac{\\partial E}{\\partial e}\\frac{\\partial e}{\\partial y^{out}}\\frac{\\partial y^{out}}{\\partial v^{out}}$\n",
    "\n",
    "* Derivative of linear combination of output layer $\\frac{\\partial v^{out}}{\\partial y^{1}} = w^{out}$\n",
    "\n",
    "* Derivative of activation function: $\\frac{\\partial y^{1}}{\\partial v^{1}} = 1 - tanh^2(v^{1}) = 1 - (y^{1})^2$\n",
    "\n",
    "* Derivative of hidden layer linear combination: $\\frac{\\partial v^{1}}{\\partial w^{1}(k)} = x^{1}$\n",
    "\n",
    "The derivative can be rewritten as, (demonstration only)\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w^{1}(k)}= \\delta^{out} w^{out} (1 - (y^{1})^2) x^{1}$$\n",
    "\n",
    "In this equation, the multiplications can cause confussion and wrong calculations. Keep in mind that we are trying to move gradients backward in the graph. So that every gradient should follow a route and multiplied by the weight of that route and accumulated. First let's show our terms and try to visualize on the graph bellow. \n",
    "$$ \\delta^{out} = \\begin{bmatrix} \\delta^{out}_0 \\\\ \\delta^{out}_1 \\end {bmatrix} \\quad w^{out} = \\begin{bmatrix} w^{out}_{0,0} & w^{out}_{0,1} & w^{out}_{0,2} \\\\ w^{out}_{1,0} & w^{out}_{1,1} & w^{out}_{1,2} \\end {bmatrix}  $$\n",
    "Here bias terms are the weight with second underscript 2, let's remove them. Remember, that bias terms always 1, they are not effected by previous layers or do not effect the previous layers.\n",
    "$$ \\delta^{out} = \\begin{bmatrix} \\delta^{out}_0 \\\\ \\delta^{out}_1 \\end {bmatrix} \\quad w^{out} = \\begin{bmatrix} w^{out}_{0,0} & w^{out}_{0,1} \\\\ w^{out}_{1,0} & w^{out}_{1,1} \\end {bmatrix}  $$\n",
    "\n",
    "![Local Gradients](local_gradients.png)\n",
    "\n",
    "From the graph above, the accumulation of the weight multiplied local gradients can be seen. So the equation becomes,\n",
    "\n",
    "$$ (w^{out})^T \\delta^{out} = \\begin{bmatrix} w^{out}_{0,0} & w^{out}_{1,0} \\\\ w^{out}_{0,1} & w^{out}_{1,1} \\end {bmatrix} \\begin{bmatrix} \\delta^{out}_0 \\\\ \\delta^{out}_1 \\end {bmatrix} = \\begin{bmatrix} w^{out}_{0,0}\\delta^{out}_0 + w^{out}_{1,0}\\delta^{out}_1\\\\ w^{out}_{0,1}\\delta^{out}_0 + w^{out}_{1,1}\\delta^{out}_1\\ \\end {bmatrix} $$\n",
    "\n",
    "Now let's add the derivative of the activation function. This term is the local gradient of layer 1.\n",
    "\n",
    "$$ \\delta^{1} = (w^{out})^T \\times \\delta^{out}\\circ (1 - (y^{1})^2)$$\n",
    "\n",
    "Respectively, the local gradient of layer 0 is,\n",
    "\n",
    "$$ \\delta^{0} = (w^{1})^T \\times \\delta^{1}\\circ (1 - (y^{0})^2)$$\n",
    "\n",
    "The activation function elements are element-wise multiplication.\n",
    "\n",
    "Finally derivatives are defines as,\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w^{out}(k)}= \\delta^{out} \\times (x^{out})^T \\quad \\frac{\\partial E}{\\partial w^{1}(k)}= \\delta^{1} \\times (x^{1})^T \\quad \\frac{\\partial E}{\\partial w^{0}(k)}= \\delta^{0} \\times (x^{0})^T $$\n",
    "\n",
    "Keep in mind that bias terms are removed when calculating local gradients and $x^0$is the input of the neural network. Now the partial derivatives are computed by using local gradients.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 2) (2, 1)\n(2, 1)\n(4, 2) (2, 1)\n(4, 1)\n(4, 1)\n"
     ]
    }
   ],
   "source": [
    "local_gradient_out = error * -1 * (1 - output_out**2)\n",
    "\n",
    "weight_out_temp = np.transpose(weights_out[:, :-1]) #no bias and transpose\n",
    "print(weight_out_temp.shape, local_gradient_out.shape)\n",
    "temp1 = np.matmul(weight_out_temp, local_gradient_out)\n",
    "print(temp1.shape)\n",
    "local_gradient_1 = temp1 * (1 - output_1**2)\n",
    "\n",
    "weight_1_temp = np.transpose(weights_1[:, :-1]) #no bias and transpose\n",
    "print(weight_1_temp.shape, local_gradient_1.shape)\n",
    "temp0 = np.matmul(weight_1_temp, local_gradient_1)\n",
    "print(temp0.shape)\n",
    "local_gradient_0 = temp0 * (1 - output_0**2)\n",
    "print(local_gradient_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 1) (3, 1)\n(2, 3)\n(2, 1) (5, 1)\n(2, 5)\n(4, 1) (4, 1)\n(4, 4)\n"
     ]
    }
   ],
   "source": [
    "print(local_gradient_out.shape, input_out.shape)\n",
    "derivative_out = np.matmul(local_gradient_out, input_out.transpose())\n",
    "print(derivative_out.shape)\n",
    "\n",
    "print(local_gradient_1.shape, input_1.shape)\n",
    "derivative_1 = np.matmul(local_gradient_1, input_1.transpose())\n",
    "print(derivative_1.shape)\n",
    "\n",
    "print(local_gradient_0.shape, input_0.shape)\n",
    "derivative_0 = np.matmul(local_gradient_0, input_0.transpose())\n",
    "print(derivative_0.shape)"
   ]
  },
  {
   "source": [
    "### Weight Update with momentum\n",
    "\n",
    "The weights can be updated by using the equaion,\n",
    "$$w(k+1) = w(k) - lr \\frac{\\partial E}{\\partial w(k)} + m (w(k) - w(k-1))$$\n",
    "$lr$ is learning rate,  \n",
    "$m$ is the momentum coefficient."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "lr = 1e-3\n",
    "m = 0.9\n",
    "old_weights_0 = weights_0\n",
    "old_weights_1 = weights_1\n",
    "old_weights_out = weights_out\n",
    "\n",
    "#Update without momentum\n",
    "weights_0 = weights_0 - lr * derivative_0\n",
    "weights_1 = weights_1 - lr * derivative_1\n",
    "weights_out = weights_out - lr * derivative_out\n",
    "\n",
    "#Update with momentum\n",
    "weights_0 = weights_0 - lr * derivative_0 + m * (weights_0 - old_weights_0)\n",
    "weights_1 = weights_1 - lr * derivative_1 + m * (weights_1 - old_weights_1)\n",
    "weights_out = weights_out - lr * derivative_out + m * (weights_out - old_weights_out)\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 7,
   "outputs": []
  }
 ]
}